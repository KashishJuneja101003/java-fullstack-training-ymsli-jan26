Kafka Notes ‚Äì Developer Friendly Version
----------------------------------------
	Introduction to Kafka 
	Kafka Installation (Windows/Linux)
	Kafka CLI commands
	Kafka + Spring Boot Hello World
	Producer & Consumer with custom objects
	Java-based Kafka Config
	Kafka Exception Handling
	Kafka Streams (Intro)


KAFKA FUNDAMENTALS
---------------------------
Kafka = distributed log-based messaging system.

Core concepts:
---------------
Topic = logical message channel
Partition = parallel log (enables scaling)
Offset = unique position of message
Producer = writes messages
Consumer = reads messages
Consumer Group = group of consumers sharing load
Ordering = per partition, not across topic
Scalability = achieved by increasing partitions

Kafka guarantees:
------------------------
Ordered messages per partition
Durability via replication
High throughput


Step by step configuration
-----------------------------

Download kafka:
-------------
https://archive.apache.org/dist/kafka/2.8.0/kafka_2.13-2.8.0.tgz

https://archive.apache.org/dist/kafka/2.8.0/

change: server.properties
log.dirs=c:/kafka/kafka-logs


change : zookeeper.properties
dataDir=c:/kafka/zookeeper


Kafka installation on Window:
-------------------------------
1. Start Zookeeper(port 2181)
-------------------------------
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

2. Start Kafka Broker (9090)
-----------------------------
.\bin\windows\kafka-server-start.bat .\config\server.properties

3. Create topic
----------------
	Topic: communication chennal on which producer put the messages and consumer consume the the data
	for performance consideration topic divided into partitions 
	If any partition is not working we keep replication

go to window:

.\kafka-topics.bat --bootstrap-server localhost:9092 --create --topic t-hello2 --partitions 3 --replication-factor 1

.\kafka-topics.bat --bootstrap-server localhost:9092 --create --topic order-confirmed --partitions 3 --replication-factor 1

.\kafka-topics.bat --bootstrap-server localhost:9092 --create --topic orders --partitions 3 --replication-factor 1

.\kafka-topics.bat --bootstrap-server localhost:9092 --create --topic shipment  --partitions 3 --replication-factor 1

List topic

.\kafka-topics.bat --bootstrap-server localhost:9092 --list

describe topic
.\kafka-topics.bat --bootstrap-server localhost:9092 --describe --topic  t-hello2

delete topic
.\kafka-topics.bat --bootstrap-server localhost:9092 --delete --topic t-hello2

4. Start Producer
--------------------
.\kafka-console-producer.bat --broker-list localhost:9092 --topic  t-hello2

Send message
How are you

5> Receive message
-------------------
.\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic  t-hello2 --from-beginning
How are you


Spring boot kafka hello world:
--------------------------------
step 1. Start Zookeeper(port 2181)
-------------------------------

.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

step 2. Start Kafka Broker (9090)
-----------------------------
.\bin\windows\kafka-server-start.bat .\config\server.properties



step 3: create simple producer and consumer project : 
-------------------------------------------------
producer:
---------
@Service
public class OrderService {

@Autowired
private KafkaTemplate<String, String> kafkaTemplate;

public void produce(String orderString) {  
    System.out.println("order is created");  
    kafkaTemplate.send("t-hello2", orderString);  
}  


}

@RestController
public class OrderController {

	@Autowired  
	private OrderService orderService;  

	@GetMapping("/orders")  
	public String send(@RequestParam String order) {  
		orderService.produce(order);  
		return "ok";  
	}  

}

server.port=8080

http://localhost:8080/orders?item=laptop 

consumer:
----------
@Service
public class OrderConsumer {
	@KafkaListener(topics = "t-hello2", groupId = "order-group")
	public void consume(String orderString) {
		System.out.println(orderString);
		}
}

server.port=8081


step 4: how to create topic programmatically with no of partitions:
------------------------------------------------------------------

step 4.1: creating kafkaConfig:
------------------------------
@Configuration
public class KafkaConfig {
    @Bean
    public NewTopic newTopic(){
		//public NewTopic(String name, int numPartitions, short replicationFactor)
        return new NewTopic("my_topic2_sb2",3, (short) 1);
    }
}

 return TopicBuilder.name("order-events")
                           .partitions(3)
                           .replicas(1)
                           .build();
						   
What it do?
-------------
snippet is part of a Spring Boot Kafka application and serves to 
automatically create a Kafka topic when the application starts.

1. Spring Boot Kafka Auto Configuration
------------------------
When your application starts, Spring Boot initializes a KafkaAdmin bean (if you have spring-kafka in classpath).
It detects the NewTopic bean and uses AdminClient from Kafka to send a CreateTopicsRequest.

2. Topic is Created
------------------
If the topic "my_topic2_sb2" does not exist, Kafka will create it.
If it already exists, it will silently skip (by default, no error is thrown).




step 4.2: change service layer to get CompletableFuture
------------------------------------------------
@Service
public class OrderService {

	@Autowired
	private KafkaTemplate<String, String> kafkaTemplate;

	public void produce(String orderString) {  
			CompletableFuture<SendResult<String, String>> future =
					template.send("my_topic2_sb", message);

			future.whenComplete(((result, ex) -> {
				if(ex==null){
					System.out.println(result.getRecordMetadata().hasOffset());//:)
				}else {
					System.out.println(ex.getMessage());
				}
			}));
		}
}

What happens internally?
--------------------------
 KafkaTemplate.send(...)
 -----------------------
		This method asynchronously sends a message to Kafka.

		Returns a CompletableFuture<SendResult>, which represents a promise of future completion.

		Under the hood, this is backed by KafkaProducer.send(ProducerRecord, Callback) 
		‚Äî which uses a non-blocking network I/O thread.

		 It doesn‚Äôt block the current thread waiting for the Kafka response.
		 
		Imagine you're sending a courier package: syn vs asynchronously
		----------------------------------------------------------------------

		Synchronous: You wait at the counter until the delivery guy returns and confirms.

		Asynchronous (what you‚Äôre doing): You hand it off to the courier and go back to your work. 
		When delivery is done, you get an SMS or email (callback).


Fast HTTP response (202 Accepted).Kafka result is processed in background.
-------------------------------------------------------------------
@PostMapping("/opublic void produce(String orderString) ")
public ResponseEntity<Void> produce(@RequestBody String msg) {
    kafkaService.processProduct(msg);
    return ResponseEntity.accepted().build(); // returns immediately
}


step 4.3: change controller to producer 5000 messages
-----------------------------------------------------
@RestController
public class ProductController {
  
	//..........
    @GetMapping(path = "producer/{message}")
    public String processOrder(@PathVariable String orderString){
        for(int i=0;i<5000;i++){
            productService.processProduct(orderString+" "+i);
        }
        return "order is processed";
    }
}


how to send message to specfic partition and recive at specfic partition

Note: working with specific Kafka partitions for precise control over message flow
--------------------------------------------------------------------------------

Before understanding what is happing here: first revise terms:
---------------------------------------------
Before diving into your code:
Term					Meaning
-----------------------------------------------
Topic				A named stream of data (e.g., "busycoder-demo")
Partition			A topic is split into multiple partitions to enable parallelism and scalability
Producer			Sends messages to a topic (possibly targeting a specific partition)
Consumer			Reads messages from one or more partitions
Consumer Group		A set of consumers that share load across partitions




how to read message from a spefic partition
-----------------------------------------------

@KafkaListener(topics = "busycoder-demo", groupId = "my_topic_group_id", topicPartitions
		= {@TopicPartition(topic = "busycoder-demo", partitions = {"2"})})
		

This tells Spring Kafka:
-------------------------------
	‚ÄúI only want to consume messages from partition 2 of topic busycoder-demo.‚Äù
	Other partitions (e.g., 0, 1, 3...) are ignored by this method.

	The consumer joins the group my_topic_group_id, 
	but Spring overrides Kafka's automatic partition assignment.
	
	This is manual partition assignment, and it bypasses Kafka's load balancing.
	
how to send messages to the specific partition
----------------------------------------------
kafkaTemplate.send("busycoder-demo", 3, null(key), data);

we are telling Kafka to:
--------------------------
Send message to topic = busycoder-demo
Write to partition 3
No key (null)
Payload = data

Important:
----------
	Manual partition assignment disables Kafka‚Äôs load balancing
	Use carefully

step 5: kafka consumer/producer custom objects
-----------------------------------------------

step 5.1: create dto and apply annotations
-------------------------------------------
public class Order {
	private String id;
	private String name;
	private double price;
	// add fields as needed
}
step 5.2: change service layer 
-------------------------------
@Service
public class OrderService {
	
	@Autowired
	private KafkaTemplate<String, Order>kafkaTemplate;
	
	public void bookOrder(Order order) {
		System.out.println("Order is send....");
		kafkaTemplate.send("my_topic", order);
	}
}

step 5.3: change controller layer 
-------------------------------
@RestController
public class OrderController {

	@Autowired
	private OrderService orderService;
	@PostMapping("orders")
	public String bookOrder(@RequestBody Order order) {
		produceService.bookOrder(order);
		return "order added";
	}
}

step 5.4: change configuration
-------------------------------
server:
  port: 8080
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer



Kafka consumer:
-------------------

public class Order {
	private String id;
	private String name;
	private double price;
	// add fields as needed
}


@Service
public class ShipmentService {
	@KafkaListener(topics = "my_topic", groupId = "my_topic_group_id")
	public void consume(Order order) {
		System.out.println(order);
	}
}

server:
  port: 8081
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: "*"


Note:
Rather using dto we can use string, but not prefered
-------------------------------------
Producer:
String json = objectMapper.writeValueAsString(order);
kafkaTemplate.send("orders", json);
Consumer:
Order order = objectMapper.readValue(jsonString, Order.class);


step 6: Spring Boot with Kafka Producer Example with java configuration
--------------------------------------------------------------
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaProducerConfig {

    @Bean
    public NewTopic createTopic(){
        return new NewTopic("busycoder-demo", 3, (short) 1);
    }

    @Bean
    public Map<String,Object> producerConfig(){
        Map<String,Object> props=new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
                "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                JsonSerializer.class);
        return props;
    }

    @Bean
    public ProducerFactory<String,Object> producerFactory(){
        return new DefaultKafkaProducerFactory<>(producerConfig());
    }

    @Bean
    public KafkaTemplate<String,Object> kafkaTemplate(){
        return new KafkaTemplate<>(producerFactory());
    }

}


KafkaConsumerConfig
---------------------
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.config.KafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;
import org.springframework.kafka.support.serializer.JsonDeserializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConsumerConfig {

    @Bean
    public NewTopic createTopic(){
        return new NewTopic("busycoder-demo", 3, (short) 1);
    }
    @Bean
    public Map<String, Object> consumerConfig() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
                "localhost:9092");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
                JsonDeserializer.class);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        return props;
    }

    @Bean
    public ConsumerFactory<String,Object> consumerFactory(){
        return new DefaultKafkaConsumerFactory<>(consumerConfig());
    }

    @Bean
    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, Object>>
        kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        return factory;
    }
}



Kafka exception handling
---------------------------
What is Kafka Exception Handling?
------------------------------------
When does it matter?
	Kafka consumers can fail while processing messages due to:
	Invalid data
	Downstream service failure (DB, API)
	Serialization errors
	Business rules (e.g. product price > ‚Çπ1,00,000)

In such cases, by default, Spring Kafka:
	Retries in the same poll cycle (not configurable by delay)
	Skips the message and moves on after max failures (not ideal)

	No built-in DLQ support unless configured

Problem with Default Behavior:
	No delay between retries.
	No max attempt limit across polls.
	No dead-letter queue (DLT) for failed messages.




What is a Dead Letter Topic (DLT)?
---------------------------------
DLT = Graveyard for Bad Messages
	If a message fails all retry attempts (e.g. 3 retries), 
	it is automatically moved to a Dead Letter Topic. This allows you to:

	Analyze faulty messages later.
	Prevent stuck consumers.
	Alert operations team.
	Fix bad data and reprocess.


consider consumer and producer:
--------------------------
producer:
----------
@RestController
@RequestMapping("/orders")
public class OrderController {

    @Autowired
    private KafkaTemplate<String, Order> kafkaTemplate;

    @Value("${app.topic.name}")
    private String topic;

    @PostMapping
    public String bookOrder(@RequestBody Order order) {
        kafkaTemplate.send(topic, order);
        return "order sent!";
    }
}

kafka consumer:
--------------
@Service
@Slf4j
public class KafkaMessageConsumer {

    @RetryableTopic(
        attempts = "3",
        backoff = @Backoff(delay = 3000, multiplier = 2.0),
        autoCreateTopics = "true",
        exclude = {NullPointerException.class}
    )
    @KafkaListener(topics = "${app.topic.name}", groupId = "busycoder-group")
    public void consumeEvents(Order order,
        @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
        @Header(KafkaHeaders.OFFSET) long offset) {

        log.info("Received: {}, Price: {}", order.getName(), order.getPrice());

        // Simulated error
        if (order.getPrice() > 100000) {
            throw new IllegalArgumentException("Price too high!");
        }
    }

    @DltHandler
    public void handleDlt(Product product,
        @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
        @Header(KafkaHeaders.OFFSET) long offset) {

        log.warn("üíÄ Dead Letter Received: {}, Topic: {}, Offset: {}", product.getName(), topic, offset);
    }
}


When using @RetryableTopic, Spring Kafka creates:
------------------------------------------------
Retry topics:
e.g., my-topic-retry-0, my-topic-retry-1, my-topic-retry-2

DLT topic:
e.g., my-topic-dlt

These are automatically created unless you disable auto-topic-creation.

What Happens Internally?
------------------------------------------
Stage					Description
---------------------------------------------
First Attempt			Message is consumed from app.topic.name.

Retry					If RuntimeException occurs, Spring will retry up to 3 more times (total 4 attempts).

Backoff					Retries happen with exponential backoff: 3s, 4.5s, 6.75s... up to 15s.

Skip Exception			If NullPointerException occurs, it's not retried. Directly goes to DLT.

DLT						After final failure, the message is published to app.topic.name-dlt.

DLT Consumer			@DltHandler receives this failed message for monitoring, alerting, or fixing.




How are Retry Topics Structured Internally?
-------------------------------------------------
Assuming your original topic is products, Spring will auto-create:
-------------------------------------------
products                  --> original topic
products-retry-0         --> for 1st retry
products-retry-1         --> for 2nd retry
products-retry-2         --> for 3rd retry
products-dlt             --> for failed messages
Each retry topic adds delay based on the backoff.


Full Flow Example
---------------------
Original Topic ‚Üí Consumer ‚Üí Exception
                     ‚Üì
             Retry-0 (after 3s)
                     ‚Üì
             Retry-1 (after 4.5s)
                     ‚Üì
             Retry-2 (after 6.75s)
                     ‚Üì
                   DLT ‚Üí @DltHandler
				   
				   
				   
Why This is Awesome for Real-World Apps
--------------------------------------
No manual retry loops
Prevents blocking consumers
Gives you auditability

Works great for:
	Payment systems
	Banking fraud events
	Order processing pipelines
	ETL

Customize DLT Topic Name
--------------------------
@RetryableTopic(
    attempts = "3",
    dltTopicSuffix = "-mydlt",
    retryTopicSuffix = "-retry-custom"
)


What‚Äôs Happening Behind the Scenes?
---------------------------------------

	If an exception is thrown in the main topic listener, Spring will:
	Retry the message up to 3 times (with exponential backoff)
	If still failing ‚Üí send to DLT (Dead Letter Topic)
	The @DltHandler listens on this DLT and logs or reprocesses as needed.

Testing
----------
Run consumer first.

Run producer and call:
-----------------------
POST http://localhost:8080/orders
{
  "name": "Car",
  "price": 150000
}

Expected:
--------------
Retry will happen 3 times
Final failure will be sent to DLT
handleDlt() will be triggered.

Benefits
-----------
	Protects system from crashes
	Allows retry with configurable delays
	Ensures faulty messages are not lost
	Separate DLT logic = clean error handling





Example: fixed rate consumer and producer:
________________________________________
@Service
public class HelloKafkaProducer {
	@Autowired
	private KafkaTemplate<String, String>kafkaTemplate;
	
	private int i=0;
	private Logger logger=LoggerFactory.getLogger(HelloKafkaProducer.class);
	
	@Scheduled(fixedRate = 1000)
	public void sendHello() {
		i++;
		kafkaTemplate.send("t_hello", "fixed rate "+ i);
	}
}



@EnableScheduling
@SpringBootApplication
public class KafkaProducerApplication implements CommandLineRunner{
}






Case Study: ‚ÄúOrder Management App using Apache Kafka (Spring Boot 3)‚Äù
=========================================================================
Objective

To build a simple Order Management microservice system using Kafka where:
    ‚Ä¢ OrderProducerService publishes messages to a topic order_topic.
    ‚Ä¢ OrderConsumerService listens to that topic and processes incoming order updates.
    ‚Ä¢ Communication is asynchronous and loosely coupled.
    ‚Ä¢ Configuration uses the same YAML Kafka setup you already have (no Docker, no Compose).

High-Level Architecture

                      +-------------------+
                      |  OrderProducerApp |
                      |  (Spring Boot)    |
                      +---------+---------+
                                |
                                | KafkaTemplate.send()
                                v
                      +-------------------+
                      |     Kafka Broker  |
                      |   (Topic: order_topic)  |
                      +---------+---------+
                                |
                                | @KafkaListener
                                v
                      +-------------------+
                      |  OrderConsumerApp |
                      |  (Spring Boot)    |
                      +-------------------+

Modules Overview
-----------------------
Component					Responsibility										Key Technology
OrderProducerApp		Accepts REST request, sends message to Kafka topic		Spring Boot, KafkaTemplate
OrderConsumerApp		Listens on order_topic, consumes order events			Spring Boot, KafkaListener
Kafka Broker			Message queue system ensuring reliable delivery			Apache Kafka

DTO Classes
------------
@Data
@AllArgsConstructor
@NoArgsConstructor
@ToString
public class Order {
    private String orderId;
    private String name;
    private int qty;
    private double price;
}


OrderStatus.java
--------------------
@Data
@AllArgsConstructor
@NoArgsConstructor
@ToString
public class OrderStatus {
    private Order order;
    private String status;  // "PROCESS", "COMPLETED"
    private String message;
}

Producer Side: OrderProducerApp
--------------------------------
Controller

@RestController
@RequestMapping("/order")
public class OrderController {

    @Autowired
    private OrderProducerService producerService;

    @PostMapping("/{restaurantName}")
    public String bookOrder(@RequestBody Order order, @PathVariable String restaurantName) {
        order.setOrderId(UUID.randomUUID().toString());
        OrderStatus orderStatus = new OrderStatus(order, "PROCESS",
                "Order placed successfully at " + restaurantName);
        producerService.sendOrder(orderStatus);
        return " Order sent to Kafka successfully!";
    }
}

Service Layer
----------------------
@Service
public class OrderProducerService {

    @Autowired
    private KafkaTemplate<String, OrderStatus> kafkaTemplate;

    private static final String TOPIC = "order_topic";

    public void sendOrder(OrderStatus orderStatus) {
        System.out.println("üì§ Sending message to Kafka: " + orderStatus);
        kafkaTemplate.send(TOPIC, orderStatus);
    }
}

application.yml (for Producer)
--------------------------------
server:
  port: 9292

spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer


Consumer Side: OrderConsumerApp
-------------------------------------
@Service
public class OrderConsumerService {

    @KafkaListener(topics = "order_topic", groupId = "order_group_id")
    public void consumeOrder(OrderStatus orderStatus) {
        System.out.println(" Message received from Kafka: " + orderStatus);
    }
}

application.yml
------------------
server:
  port: 9393

spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: order_group_id
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: "*"

ASCII Flow Diagram ‚Äî End-to-End
---------------------------------
                      +-------------------------------------------+
                      |          CLIENT (Postman / UI)            |
                      +-------------------------------------------+
                                       |
             POST /order/saravana-bhavan with Order JSON
                                       |
                                       v
                   +-----------------------------------------+
                   |         OrderProducerApp (9292)          |
                   |-----------------------------------------|
                   | OrderController.bookOrder()             |
                   |   -> Create UUID                        |
                   |   -> Build OrderStatus DTO              |
                   |   -> OrderProducerService.sendOrder()   |
                   |   -> kafkaTemplate.send(order_topic)    |
                   +-----------------------------------------+
                                       |
                                       | JSON message (serialized)
                                       v
                     +-----------------------------------+
                     |   Kafka Broker (localhost:9092)    |
                     |   Topic: order_topic               |
                     +-----------------------------------+
                                       |
                                       v
                     +-----------------------------------+
                     |       OrderConsumerApp (9393)     |
                     |-----------------------------------|
                     | @KafkaListener(order_topic)       |
                     |   -> Deserialize OrderStatus      |
                     |   -> Print / process order        |
                     +-----------------------------------+

Sample Request 
------------------
POST  http://localhost:9292/order/saravana-bhavan
Content-Type: application/json

{
  "name": "north thali",
  "qty": 2,
  "price": 350
}

Console Output
----------------------
Producer:
Sending message to Kafka: OrderStatus(order=Order(orderId=9e12..., name=north thali, qty=2, price=350.0), status=PROCESS, message=Order placed successfully at saravana-bhavan)

Order sent to Kafka successfully!

Consumer:
üì• Message received from Kafka: OrderStatus(order=Order(orderId=9e12..., 
		name=north thali, qty=2, price=350.0), status=PROCESS, message=Order placed successfully at saravana-bhavan)

Interview-Style Questions
------------------------------------------------------
1. What‚Äôs the role of KafkaTemplate in Spring Boot?
It‚Äôs a high-level API used to send messages to Kafka topics, 
	similar to RabbitTemplate in RabbitMQ.
	
2. What happens if the consumer app is down when messages are published?
	Kafka retains messages in topic partitions; consumer 
	receives them once it reconnects.
	
3. How is Kafka different from RabbitMQ in message consumption pattern?
	Kafka consumers pull data from topics, RabbitMQ pushes it to consumers.
	
4. What‚Äôs the use of groupId in KafkaListener?
	It groups consumers so messages are distributed among them ‚Äî ensures load balancing.
	
5. Can you send custom Java objects through Kafka?
	Yes, using JsonSerializer and JsonDeserializer.
	
6. How to ensure message ordering?
	Messages with the same key are always sent to the same partition.
	
7. How to handle message retries in Kafka?
	Use Dead Letter Topics (DLT) or retry mechanisms via SeekToCurrentErrorHandler.




Kafka installation on Linux:
-------------------------------
Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

Start Kafka Server
bin/kafka-server-start.sh config/server.properties

Create Kafka Topic
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic Kafka_Example

Consume from the Kafka Topic via Console
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Kafka_Example --from-beginning


Consumer:
# create topic t_hello
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic t_hello --partitions 1 --replication-factor 1

# list topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

# describe topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic t_hello

# create topic t_test
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic t_test --partitions 1 --replication-factor 1

# delete topic t_test
bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic t_test

